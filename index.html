<!DOCTYPE html>
<html class="main"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">


    <title>Nikhil Vyas</title>



	<link rel="stylesheet" href="style.css" type="text/css">

</head>
<body data-main="true">


	<div id="page">
		<div id="logo">
			<h1>Nikhil Vyas</h1>
			<p>
		  nikhil at g.harvard.edu<br>
      vyasnikhil96 at gmail.com (Preferred)<br>
	<a href="https://twitter.com/vyasnikhil96"style="color:DARKVOILET;">Twitter</a>
			</p>
		</div>

		<img src="./b.jpg" id="photo">

			<p>
        I am a postdoc at Harvard hosted by Prof. Demba Ba, Prof. Boaz Barak, Prof. Lucas Janson and Prof. Cengiz Pehlevan. Before this, I was a graduate student at the <a href="https://toc.csail.mit.edu/"style="color:DARKVOILET;"> TOC group at MIT</a> where I was advised by Prof. <a href="https://people.csail.mit.edu/rrw/"style="color:DARKVOILET;"> Ryan Williams</a>.
<br><br> My current research focusses on deep learning, most recently on optimization. 

 <br> <br>

<p></p>


<p></p><h2>Publications (ML)</h2>

<li><font size="4"><a href="https://arxiv.org/abs/2409.11321"style="color:DARKVOILET;">SOAP: Improving and Stabilizing Shampoo using Adam.</a></font><br>
Nikhil Vyas*, Depen Morwani*, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, Sham Kakade<br> </li>

<li><font size="4" style="color:BLACK;">Connections between Schedule-Free SGD, Accelerated SGD Variants, and Weight Averaging.</font><br>
Depen Morwani*, Nikhil Vyas*, Hanlin Zhang, Sham Kakade<br> </li>

<li><font size="4" style="color:BLACK;">Loss-to-Loss Prediction: Language model scaling laws across datasets.</font><br>
David Brandfonbrener, Nikhil Anand, Nikhil Vyas, Eran Malach, Sham Kakade<br> </li>

<li><font size="4" style="color:BLACK;">Mixture of Parrots: Experts improve memorization more than reasoning.</font><br>
Samy Jelassi, Clara Mohri, David Brandfonbrener, Alex Gu, Nikhil Vyas, Nikhil Anand, David Alvarez-Melis, Yuanzhi Li, Sham Kakade, Eran Malach<br> </li>

<li><font size="4" style="color:BLACK;">Understanding Critical Batch Sizes: Scheduling and Batch-Size Invariance in Data-constrained Pre-training.</font><br>
Hanlin Zhang, Depen Morwani, Nikhil Vyas, Jingfeng Wu, Difan Zou, Udaya Ghai, Dean Foster, Sham Kakade<br> </li>

<li><font size="4"><a href="https://openreview.net/forum?id=fZqMVTz7K5" style="color:DARKVOILET;">AdaMeM: Memory Efficient Momentum for Adafactor.</a></font><br>
Nikhil Vyas, Depen Morwani, Sham Kakade<br> </li>


<li><font size="4"><a href="https://openreview.net/forum?id=fZqMVTz7K5"style="color:DARKVOILET;">AdaMeM: Memory Efficient Momentum for Adafactor.</a></font><br>
Nikhil Vyas, Depen Morwani, Sham Kakade <br> </li>
		
<li><font size="4"><a href="https://arxiv.org/abs/2407.07972"style="color:DARKVOILET;">Deconstructing What Makes a Good Optimizer for Language Models.</a></font><br>
Rosie Zhao*, Depen Morwani*, David Brandfonbrener*, Nikhil Vyas*, Sham Kakade<br> </li>
		
<li><font size="4"><a href="https://arxiv.org/abs/2406.17748"style="color:DARKVOILET;">A New Perspective on Shampoo's Preconditioner.</a></font><br>
Depen Morwani*, Itai Shapira*, Nikhil Vyas*, Eran Malach, Sham Kakade, Lucas Janson<br> </li>		

<li><font size="4"><a href="https://arxiv.org/abs/2306.08590"style="color:DARKVOILET;">Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning.</a></font><br>
Nikhil Vyas*, Depen Morwani*, Rosie Zhao*, Gal Kaplun*, Sham Kakade, Boaz Barak<br> ICML 2024 (Spotlight)<br> </li>

<li><font size="4"><a href="https://arxiv.org/abs/2402.03563"style="color:DARKVOILET;">Distinguishing the Knowable from the Unknowable with Language Models.</a></font><br>
Gustaf Ahdritz*, Tian Qin*, Nikhil Vyas, Boaz Barak, Benjamin L. Edelman<br> ICML 2024<br> </li>

		
<li><font size="4"><a href="https://aclanthology.org/2023.emnlp-main.583.pdf"style="color:DARKVOILET;">On the benefits of learning to route in mixture-of-experts models.</a></font><br>
Nishanth Dikkala*, Nikhil Ghosh*, Raghu Meka*, Rina Panigrahy*, Nikhil Vyas*, Xin Wang*<br> EMNLP 2023<br> </li>

		
<li><font size="4"><a href="https://arxiv.org/abs/2305.18411"style="color:DARKVOILET;">Feature-Learning Networks Are Consistent Across Widths At Realistic Scales.</a></font><br>
Nikhil Vyas*, Alexander Atanasov*, Blake Bordelon*, Depen Morwani, Sabarish Sainathan, Cengiz Pehlevan<br> NeurIPS 2023<br> </li>

<li><font size="4"><a href="https://arxiv.org/abs/2307.12941"style="color:DARKVOILET;">On Privileged and Convergent Bases in Neural Network Representations.</a></font><br>
Davis Brown*, Nikhil Vyas*, Yamini Bansal<br> Workshop on High-dimensional Learning Dynamics at ICML 2023<br> </li>

<li><font size="4"><a href="https://arxiv.org/abs/2302.10870"style="color:DARKVOILET;">Provable Copyright Protection for Generative Models.</a></font><br>
Nikhil Vyas, Sham Kakade, Boaz Barak<br> ICML 2023<br> </li>
		
		
<li><font size="4"><a href="https://arxiv.org/abs/2206.10012"style="color:DARKVOILET;">Limitations of the NTK for Understanding Generalization in Deep Learning.</a></font><br>
Nikhil Vyas, Yamini Bansal, Preetum Nakkiran<br> TMLR<br> </li>

<li><font color="“#05a”" size="4"><a href="https://arxiv.org/abs/1812.05013"style="color:DARKVOILET;"> Thwarting Adversarial Examples: An L_0-Robust Sparse Fourier Transform.</a></font><br>
Mitali Bafna*, Jack Murtagh*, Nikhil Vyas*<br>NeurIPS 2018  <br>    </li>

<p></p><h2>Publications (Theory) </h2>

<li><font size="4"><a href="https://arxiv.org/abs/2407.12762"style="color:DARKVOILET;">Quasi-Linear Size PCPs with Small Soundness from HDX.</a></font><br>
Mitali Bafna, Dor Minzer, Nikhil Vyas<br> </li>

<li><font size="4"><a href="https://drops.dagstuhl.de/opus/volltexte/2023/17602/"style="color:DARKVOILET;">On Oracles and Algorithmic Methods for Proving Lower Bounds.</a></font><br>Nikhil Vyas, Ryan Williams<br> ITCS 2023<br> </li>

		
<li><font size="4"><a href="https://arxiv.org/abs/2207.00104"style="color:DARKVOILET;">On the Number of Quantifiers as a Complexity Measure.</a></font><br>Ronald Fagin, Jonathan Lenchner, Nikhil Vyas, Ryan Williams<br> MFCS 2022<br> </li>
		
		
<li><font size="4"><a href="https://arxiv.org/abs/2106.13210"style="color:DARKVOILET;">Optimal Fine-grained Hardness of Approximation of Linear Equations.</a></font><br>
Mitali Bafna, Nikhil Vyas<br> ICALP 2021<br> </li>
		
<li><font size="4"><a href="https://arxiv.org/abs/2104.14709"style="color:DARKVOILET;">Multi-Structural Games and Number of Quantifiers.</a></font><br>
Ronald Fagin, Jonathan Lenchner, Kenneth W. Regan, Nikhil Vyas<br> LICS 2021<br> </li>				
		
<li><font size="4"><a href="https://arxiv.org/abs/2011.03819"style="color:DARKVOILET;">Fast Low-Space Algorithms for Subset Sum.</a></font><br>
Ce Jin, Nikhil Vyas, R. Ryan Williams<br> SODA 2021<br> </li>		
		
<li><font size="4"><a href="https://arxiv.org/abs/2001.07788"style="color:DARKVOILET;">Lower Bounds Against Sparse Symmetric Functions of ACC Circuits: Expanding the Reach of #SAT Algorithms.</a></font><br>
Nikhil Vyas, R. Ryan Williams<br> STACS 2020<br> </li>

<li><font size="4"><a href="https://drops.dagstuhl.de/opus/volltexte/2020/11898/"style="color:DARKVOILET;">Near-Optimal Complexity Bounds for Fragments of the Skolem Problem.</a></font><br>
S. Akshay, Nikhil Balaji, Aniket Murhekar, Rohith Varma, Nikhil Vyas<br> STACS 2020<br> </li>

<li><font size="4"><a href="https://drops.dagstuhl.de/opus/volltexte/2020/11696/"style="color:DARKVOILET;">Algorithms and Lower Bounds for Cycles and Walks: Small Space and Sparse Graphs.</a></font><br>
	Andrea Lincoln, Nikhil Vyas<br> ITCS 2020<br> </li>

<li><font size="4"><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-45724-2_6"style="color:DARKVOILET;">Efficient Constructions for Almost-Everywhere Secure Computation.</a></font><br>
Siddhartha Jayanti, Srinivasan Raghuraman, Nikhil Vyas<br> EUROCRYPT 2020<br> </li>

<li><font size="4"><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-24258-9_28"style="color:DARKVOILET;">On Super Strong ETH.</a></font><br>
Nikhil Vyas, Ryan Williams<br> SAT 2019 <b> [Best Paper Award] </b> <br> </li>


<li><font size="4"><a href="https://arxiv.org/abs/1907.08185"style="color:DARKVOILET;">Imperfect Gaps in Gap-ETH and PCPs.</a></font><br>
Mitali Bafna, Nikhil Vyas <br> CCC 2019<br> </li>
<li><font size="4"><a href="https://arxiv.org/abs/1904.11606"style="color:DARKVOILET;">Approximation Algorithms for Min-Distance Problems.</a></font><br>
Mina Dalirrooyfard, Virginia Vassilevska Williams, Nikhil Vyas, Nicole Wein, Yinzhan Xu, Yuancheng Yu<br> ICALP 2019  <br> </li>

<li><font size="4"><a href="https://arxiv.org/abs/1904.11601"style="color:DARKVOILET;">Tight Approximation Algorithms for Bichromatic Graph Diameter and Related Problems.</a></font><br>
Mina Dalirrooyfard, Virginia Vassilevska Williams, Nikhil Vyas, Nicole Wein<br> ICALP 2019  <br> </li>




<li><font size="4"><a href="https://arxiv.org/abs/1804.09341"style="color:DARKVOILET;">Distribution-based objectives for Markov Decision Processes.</a></font><br>
  S. Akshay, Blaise Genest, Nikhil Vyas<br> LICS 2018  <br> </li>
<li><font color="#05a" size="4"><a href="https://drops.dagstuhl.de/opus/volltexte/2017/8130/"style="color:DARKVOILET;">Complexity of Restricted Variants of Skolem and Related Problems.</a></font><br>
  S. Akshay, Nikhil Balaji, Nikhil Vyas <br> MFCS 2017<br> </li>
<li><font color="#05a" size="4"><a href="https://arxiv.org/abs/1612.02788"style="color:DARKVOILET;"> Faster space-efficient algorithms for subset sum and k-sum.
</a></font><br> Nikhil Bansal, Shashwat Garg, Jesper Nederlof, Nikhil Vyas<br>STOC 2017<br></li>
<li><font color="#05a" size="4"><a href="https://drops.dagstuhl.de/opus/volltexte/2016/5709/"style="color:DARKVOILET;">On Regularity of Unary Probabilistic Automata.</a></font><br>
  S. Akshay, Blaise Genest, Bruno Karelovic, Nikhil Vyas<br>STACS 2016<br> </li>

<p></p><h2>Teaching</h2>
<li><font size="4"> Advanced Complexity Theory. (MIT 6.841, Spring 2022)</font><br>Teaching assistant to Professor Ryan Williams<br></li>
<li><font size="4"> Math for Computer Science. (MIT 6.042, Spring 2021)</font><br>Teaching assistant to Zachary Abel, Professors Nancy Lynch and Ryan Williams<br></li>
<li><font size="4"> <a href="https://stellar.mit.edu/S/course/6/sp19/6.890/materials.html"style="color:DARKVOILET;">Learning Augmented Algorithms. </a>(MIT 6.890, Spring 2019)</font><br>Teaching assistant to Professors Costis Daskalakis and Piotr Indyk<br></li>
<li><font size="4"> Introduction to Computational Complexity. (IIT Bombay CS 721, Fall 2016)</font><br>Teaching assistant to Professor Nutan Limaye.<br></li>
<li><font size="4"> Discrete Structures. (IIT Bombay CS 207, Fall 2015)</font><br>Teaching assistant to Professor S. Akshay.<br></li>


</div></div></div></body></html>
